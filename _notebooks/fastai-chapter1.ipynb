{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 1 - the basics\n",
    "## Machine learning vs Traditional program\n",
    "\n",
    "- You have to write the program with if conditions and loops and you get desired results\n",
    "\n",
    "    This is how a traditional program looks:\n",
    "    \n",
    "    ![traditional program](images/blog1/program.png)\n",
    "\n",
    "- In machine learning, you don't write the exact steps needed to solve a problem. Instead, simply show the computer examples of the problem to solve and let if figure out how to solve itself.\n",
    "\n",
    "    This is how machine learning program looks:\n",
    "    \n",
    "    ![traditional program](images/blog1/ml.png)\n",
    "\n",
    "- Program has now changed to model\n",
    "- Inputs are the inputs to the program - values that it processes to produce results (example - image pixels as input, and returning classification \"dog\" as a result)\n",
    "- Weight are \"other values\" that define how the program will operate. Weights are now called \"Parameters\"\n",
    "- The main idea is that the weights need to be adjusted to make the 'actual performance' of the model better \n",
    "- So, there needs to be a ** *mechanism for altering the weight assignments so as to maximize the performance* **\n",
    "\n",
    "    This is how machine learning ***training loop*** looks:\n",
    "    \n",
    "    ![training loop](images/blog1/training_loop.png)\n",
    "\n",
    "- And once the model is trained, it just behaves as a traditional program.\n",
    "\n",
    "    Final, trained model:\n",
    "\n",
    "    ![trained model](images/blog1/trained_model.png)\n",
    "    \n",
    "## Neural network\n",
    "- Neural network is a function that is so flexible that it could be used to solve any given problem, just by varying its weights.\n",
    "- *Universal approximation theorem* shows that this function can solve any problem to any level of accuracy, in theory\n",
    "- One would think that automatically updating the weights for every problem would be challenging, but there is a way to do that too. This is called **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "## Deep learning\n",
    "\n",
    "This is how the deep learning training loop looks:\n",
    "\n",
    "![deep learning training loop](images/blog1/deep_learning_loop.png)\n",
    "\n",
    "    - model = architecture\n",
    "    - weights = parameters\n",
    "    - predictions are calculated from independent variables (data, not including labels)\n",
    "    - results of the model = predictions\n",
    "    - measure of performance = loss\n",
    "    - loss depends on predictions and correct labels (dependent variables)\n",
    "\n",
    "Terms and definitions\n",
    "\n",
    "![terms and definitions](images/blog1/terms.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}